###############################################################################
# PLEASE READ 'hoaxy/data/manuals/sites.readme.md'
###############################################################################
#### Examples
###############################################################################

### site factcheck.org
  # required, name of site
- name: factcheck.org
  # required, primary domain of factcheck.org
  domain: factcheck.org
  # required, type of this site, it is a fact checking site
  site_type: fact_checking
  # base URL, default by infer, the home of factcheck.org
  base_url: http://www.factcheck.org/
  # site tags, default [], more about this site
  site_tags: []
  # alternate domains, default [], secondary domains
  # that redirect to the primary domain
  alternate_domains: []
  # is factcheck.org is alive, default true
  is_alive: true
  # is factcheck.org is enabled, default true
  # when false, this site will not be tract and lucene search may ignore it
  is_enabled: true
  # rules of how to crawl factcheck.org
  article_rules:
    # regular expression of url we like to collect
    # right now this field does not used, please ignore
    url_regex: ^http://www\.factcheck.org/20[0-2]\d/((0[1-9])|(1[0-2]))/[^/\s]+/?$
    # how to fetch the new articles update from factcheck.org
    # by default, page.spider, which crawls the home page of this site
    update:
      # here we use feed.spider as we have RSS feed URL,
      # see hoaxy.crawl.spiders
    - spider_name: feed.spider
      # the necessary parameters for building spider instance
      spider_kwargs:
        # here, we need the RSS feed URLs
        urls:
        - http://www.factcheck.org/feed/
        # and also who providse the RSS feed
        # normally the website itself, sometimes a third party, e.g. feedburner
        provider: self
    # how to fetch archive of factcheck.org
    # by default, site.page, which crawls the whole site.
    archive:
      # here we use page_template.spider, as factcheck.org use a page
      # template to list all of its posted articles
    - spider_name: page_template.spider
      spider_kwargs:
        # a list of xpaths to extract links (to find @href)
        # by default, a python tuple('/html/body',) is used
        # to fetch all links in this page
        # here, we use specified xpath expression
        # Note: please do not include /a/@href part
        href_xpaths:
        - //article//header/h2
        # page templates of factcheck.org
        # increasing by page number
        page_templates:
        - http://www.factcheck.org/page/{p_num}
    # factcheck.org also provides sitemap.xml to help us collect all
    # links in this site
    - spider_name: sitemap.spider
      spider_kwargs:
        # these URLs could be actual sitemap URL
        # OR they could be the entry of a list of sitemap.xml files
        # the spider will follow all XML links and
        # assuming these XML file are sitemaps and extract non-xml
        # links
        urls:
        - http://www.factcheck.org/sitemap.xml
#Another site, thedcgazette.com
- name: thedcgazette.com
  domain: thedcgazette.com
  site_type: claim
  base_url: http://thedcgazette.com/
  site_tags:
  - source: fakenewswatch.com
    name: hoax
  alternate_domains:
  - is_alive: true
    name: dcgazette.com
  is_alive: true
  is_enabled: true
  article_rules:
    url_regex:
    update:
    - spider_name: feed.spider
      spider_kwargs:
        urls:
        - http://dcgazette.com/feed/
        provider: self
    archive:
    - spider_name: page_template.spider
      spider_kwargs:
        href_xpaths:
        - //div[@id="main-content"]/article/header/h3
        page_templates:
        - http://dcgazette.com/page/{p_num}/
